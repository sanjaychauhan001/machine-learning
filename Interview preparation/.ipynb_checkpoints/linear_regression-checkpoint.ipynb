{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is linear regression?\n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal is to find a linear equation that best predicts the dependent variable based on the values of the independent variables.\n",
    "\n",
    "**************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain the difference between simple and multiple linear regression.\n",
    "\n",
    "In simple linear regression we have only one independent feature and in multiple linear regression we have more than 1 independent variable\n",
    "**************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the linear regression equation?\n",
    "\n",
    "Y=β0+β1X1+β2X2+…+βnXn+ϵ\n",
    "where Y  is the dependent variable, X1,X2,…,Xn are the independent variables, β0 is the intercept, β1,β2,…,βn  are the coefficients, and ϵ\\epsilonϵ is the error term.\n",
    "**************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What do the coefficients in a linear regression model represent?\n",
    "\n",
    "The coefficients in a linear regression model represent the estimated effect of each independent variable on the dependent variable, holding all other variables constant. Specifically:\n",
    "1.\tIntercept: This is the expected value of the dependent variable when all independent variables are zero.\n",
    "2.\tSlope coefficients: Each of these represents the change in the dependent variable for a one-unit increase in the corresponding independent variable, assuming all other variables remain constant\n",
    "**************************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do you interpret the intercept in a linear regression model?\n",
    "\n",
    "The intercept in a linear regression model represents the expected value of the dependent variable when all independent variables are set to zero.\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the assumptions of linear regression?\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear. \n",
    "\n",
    "Independence: The observations are independent of each other. \n",
    "\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. \n",
    "\n",
    "Normality: The residuals are normally distributed. \n",
    "\n",
    "No multicollinearity: The independent variables are not highly correlated with each other. No autocorrelation: The residuals are not correlated with each other.\n",
    "****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is multicollinearity, and how can it affect a linear regression model?\n",
    "\n",
    "Multicollinearity refers to a situation in linear regression where two or more independent variables are highly correlated with each other.it becomes challenging to determine the effect of single variable on dependent variable since increase in one independent variable will effect the another correlated independent variable. so will not get the exact relation between one variable with the dependent variable. also it will increase the risk of overfitting.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How can you detect and handle multicollinearity?\n",
    "\n",
    "Methods to detect multicollinearity include:\n",
    "\n",
    "Variance Inflation Factor (VIF), Correlation matrix\n",
    "\n",
    "To address multicollinearity, you might:\n",
    "\n",
    "Remove one of the correlated variables, Combine correlated variables, Use regularization techniques (like ridge regression)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is heteroscedasticity,effect of heteroscedasticity and how can it be detected?, how to fix Heteroscedasticity.\n",
    "\n",
    "Heteroscedasticity refers to the situation in a regression model where the variability of the residuals (the differences between observed and predicted values) is not constant across all levels of the independent variables.\n",
    "\n",
    "Effects of heteroscedasticity:\n",
    "\n",
    "Inefficient estimates: While coefficient estimates remain unbiased, they are no longer the most efficient (lowest variance) estimates.\n",
    "\n",
    "Biased standard errors: This leads to incorrect confidence intervals and hypothesis tests.\n",
    "\n",
    "Unreliable F-tests and t-tests: These tests become less reliable, potentially leading to incorrect conclusions about the significance of variables.\n",
    "\n",
    "Detection methods:\n",
    "\n",
    "Residual plots: Plot residuals against fitted values or independent variables.\n",
    "\n",
    "Breusch-Pagan test: Tests whether the variance of the errors depends on the values of the independent variables.\n",
    "\n",
    "The null hypothesis (H0): Signifies that Homoscedasticity is present.\n",
    "\n",
    "The alternative hypothesis: (Ha): Signifies that the Homoscedasticity is not present.\n",
    "\n",
    "Transform the dependent variable: We can alter the dependent variable using some technique. For example, we can take the log of the dependent variable.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is autocorrelation, and how does it affect linear regression models?\n",
    "\n",
    "Autocorrelation refers to the correlation between the error terms of a regression model at different time points or observations. It commonly occurs in time series data but can also appear in other types of data.\n",
    "\n",
    "Effects of autocorrelation on linear regression models:\n",
    "\n",
    "Biased standard errors:\n",
    "\n",
    "Usually, autocorrelation leads to underestimated standard errors.\n",
    "Due to biased standard errors, confidence intervals and hypothesis tests become unreliable.\n",
    "\n",
    "Detection methods:\n",
    "\n",
    "Durbin-Watson test:\n",
    "\n",
    "Tests for first-order autocorrelation.\n",
    "Values range from 0 to 4, with 2 indicating no autocorrelation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain R-squared and adjusted R-squared.\n",
    "\n",
    "R-squared: An R-squared of 0.7 means that 70% of the variability in the dependent variable is explained by the model.\n",
    "R-squared always increases (or stays the same) when you add more variables to the model, even if these variables are not meaningful.\n",
    "\n",
    "Adjusted R-squared\n",
    "It increases only if the new term improves the model more than would be expected by chance.\n",
    "It can decrease if a predictor improves the model less than expected by chance.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What is stepwise regression?\n",
    "\n",
    "It's used for variable selection, particularly when dealing with a large number of potential independent variables. There are three main types of stepwise regression:\n",
    "\n",
    "Forward Selection:\n",
    "\n",
    "Starts with no variables in the model.\n",
    "Adds the most significant variable (usually based on F-statistics or p-values) at each step.\n",
    "Continues adding variables until no remaining variable meets the specified criteria for entry.\n",
    "\n",
    "\n",
    "Backward Elimination:\n",
    "\n",
    "Starts with all candidate variables in the model.\n",
    "Removes the least significant variable at each step.\n",
    "Continues removing variables until all remaining variables meet the specified criteria to stay in the model.\n",
    "\n",
    "\n",
    "Bidirectional Elimination (Stepwise Regression):\n",
    "\n",
    "Combines forward selection and backward elimination.\n",
    "Variables can be added or removed at each step.\n",
    "Reassesses variables added in earlier steps, potentially removing them if they become non-significant.\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What is regularization in the context of linear regression?\n",
    "\n",
    "Regularization in the context of linear regression is a technique used to prevent overfitting and improve the generalization of the model. It does this by adding a penalty term to the loss function, which discourages the model from relying too heavily on any individual feature. The two main types of regularization for linear regression are:\n",
    "\n",
    "L1 Regularization (Lasso Regression):\n",
    "\n",
    "Adds the absolute value of the magnitude of coefficients as a penalty term.\n",
    "Can lead to sparse models by shrinking some coefficients to exactly zero.\n",
    "Loss function: L(β) = RSS + λ Σ|βj|\n",
    "\n",
    "\n",
    "L2 Regularization (Ridge Regression):\n",
    "\n",
    "Adds the squared magnitude of coefficients as a penalty term.\n",
    "Shrinks all coefficients towards zero, but typically doesn't make them exactly zero.\n",
    "Loss function: L(β) = RSS + λ Σβj²\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. What is polynomial regression, and when would you use it?\n",
    "\n",
    "Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the independent and dependent variables. It does this by including polynomial terms (squared, cubed, etc.) of the predictor variables in the regression equation.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. How can you detect and mitigate overfitting in linear regression?\n",
    "\n",
    "Train-Test Split Performance:\n",
    "\n",
    "Significant difference between training and test set performance indicates overfitting.\n",
    "The model performs much better on the training data than on the test data.\n",
    "\n",
    "R-squared vs. Adjusted R-squared:\n",
    "\n",
    "A large difference between R-squared and adjusted R-squared suggests overfitting.\n",
    "\n",
    "Mitigation Strategies:\n",
    "Regularization, Feature Selection, Increase Training Data\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
